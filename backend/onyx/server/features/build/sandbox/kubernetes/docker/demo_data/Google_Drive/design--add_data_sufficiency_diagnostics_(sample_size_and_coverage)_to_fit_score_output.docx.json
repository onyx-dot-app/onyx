{
  "id": "3ca0aabab1a246d89d5edb7eaf4e45e7",
  "semantic_identifier": "Design--Add data sufficiency diagnostics (sample size + coverage) to fit score output.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-05-10",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Add data sufficiency diagnostics (sample size + coverage) to fit score output.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview\nWe will add \u201cdata sufficiency diagnostics\u201d to the fit score generation pipeline so each fit score includes transparent indicators of whether it was computed from adequate underlying data. The backend will compute per-segment diagnostics\u2014sample size, recency, and key-signal coverage\u2014and return them alongside existing fit score outputs. The UI will use these fields to display a \u201climited data\u201d warning when one or more thresholds are not met, improving user trust and helping explain unstable or noisy scores.\n\n### Backend computation (per segment)\nDuring fit score generation, after the segment\u2019s eligible dataset is assembled but before scoring is finalized, we will compute: (1) **sample size** (e.g., number of entities/events used for that segment), (2) **recency** (e.g., age of the newest datapoint and/or percentile/mean age), and (3) **key-signal coverage** (presence/completeness of required signals used in the scoring model, such as % of entities with each required attribute, plus an aggregate coverage metric). These diagnostics will be computed using the same filters and joins as scoring to ensure consistency. Each segment will also receive a derived boolean (or enum) like `data_quality_status: OK | LIMITED`, plus a list of failing checks for precise messaging.\n\n### API contract and configurability\nWe will extend the fit score response schema to include a `diagnostics` object at the segment (or segment+score) level, containing raw metrics and threshold evaluation results (e.g., `sample_size.value`, `sample_size.threshold`, `sample_size.passed`). Thresholds will be configurable via service config (environment-based initially), with separate knobs for minimum sample size, maximum acceptable recency window, and minimum coverage per key signal and/or overall. This keeps the UI logic simple (read status + reasons) while allowing iteration on thresholds without code changes.\n\n### Logging and observability\nWe will add basic structured logging/counters whenever a segment returns `LIMITED` (including which checks failed and the computed values) and aggregate metrics to track the rate of low-data outputs over time. This will support monitoring and threshold tuning, and help identify segments consistently underpowered due to upstream data gaps. As a guardrail, we will ensure diagnostics computation is lightweight (reusing existing intermediate aggregates where possible) and does not materially increase fit score latency.",
      "link": "https://www.onyx.app/27593"
    }
  ],
  "primary_owners": [
    "brooke_spencer"
  ],
  "secondary_owners": []
}