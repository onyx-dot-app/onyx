{
  "id": "4d6c93b680674b0f9c67c99dc467e594",
  "semantic_identifier": "Design--Implement issue_prioritization score computation (frequency + severity + ARR tier) using persisted config.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-05-25",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Implement issue_prioritization score computation (frequency + severity + ARR tier) using persisted config.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "We will implement an `IssuePrioritizationScorer` module/service responsible for computing a deterministic `issue_prioritization` score for a given issue using three factors: frequency, severity, and ARR tier. At runtime, the scorer will load the *active* scoring configuration for the issue\u2019s workspace from persisted storage (e.g., `workspace_scoring_configs` with an `active_config_id` pointer or an `is_active` flag). The configuration will define per-factor weights, value-to-score mappings (or buckets), and any normalization rules (e.g., clamping, min/max scaling), enabling scoring behavior to be changed without deploys and ensuring different workspaces can tune their prioritization independently.\n\nThe scorer API will accept `(workspace_id, issue_id | issue_payload)` and return: (1) `total_score` and (2) an explainable `breakdown` object. The breakdown will include raw inputs used (e.g., observed frequency count/window, severity level, ARR tier), normalized values (if any), per-factor contribution (e.g., `frequency_score * frequency_weight`), and metadata about the config version/id applied. This is intended to be directly consumable by UI and debugging tools, and to support auditing (\u201cwhy did this issue rank above another?\u201d) with no additional computation.\n\nWe will define stable fallbacks for incomplete data. If ARR tier is missing/unknown, we will use a configurable default tier (e.g., `ARR_UNKNOWN`) with an explicit mapping in the config; if not present, fall back to a neutral multiplier/score (e.g., 1.0) to avoid dropping scores to zero unexpectedly. If severity is missing or invalid, we will use a default severity bucket (e.g., `SEV_UNSPECIFIED`) defined in config; if not present, fall back to the lowest severity contribution. All fallbacks will be encoded in the breakdown so results remain deterministic and explainable.\n\nImplementation details: the scorer will cache the active config per workspace with short TTL (and/or invalidate on config updates) to avoid repeated DB reads in hot paths. We will add unit tests covering config loading, scoring math, normalization, and all fallback branches (unknown ARR, missing severity, missing frequency), plus golden tests for breakdown shape/values. We will also add minimal instrumentation (e.g., log/metric counters for fallback usage) to detect data quality issues and unexpected config gaps.",
      "link": "https://www.onyx.app/87218"
    }
  ],
  "primary_owners": [
    "ryan_murphy"
  ],
  "secondary_owners": []
}