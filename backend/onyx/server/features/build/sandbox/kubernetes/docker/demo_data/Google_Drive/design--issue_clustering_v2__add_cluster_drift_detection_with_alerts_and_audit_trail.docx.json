{
  "id": "b0630ad82b0a4d769ffa301684ccc778",
  "semantic_identifier": "Design--issue_clustering v2: Add cluster drift detection with alerts and audit trail.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-10-28",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--issue_clustering v2: Add cluster drift detection with alerts and audit trail.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview / Goals  \nWe will add \u201ccluster drift detection\u201d to issue_clustering v2 to flag when either (a) cluster assignments change materially over time or (b) the embedding distribution for a cluster (or globally) shifts beyond configurable thresholds. When drift is detected, the system will persist a drift event record and raise a high-priority alert for triage. A lightweight audit trail will tie each drift event back to the underlying source conversations and forward to downstream engineering work (e.g., Linear issue(s)) to support defensibility and post-mortems.\n\n### Drift signals & detection approach  \nWe\u2019ll compute drift on a scheduled cadence (e.g., daily) over a rolling window of recent conversations compared to a baseline window. Two families of metrics will be implemented: **assignment drift** (e.g., % of items whose cluster changed vs prior run, cluster split/merge indicators, per-cluster churn rate) and **embedding drift** (e.g., per-cluster centroid delta, distribution divergence like KL/JS over reduced dimensions, or mean pairwise distance changes). Thresholds will be configurable per environment and optionally per cluster; we\u2019ll also support \u201cseverity\u201d bucketing (warning vs critical) to drive alert priority. Detection will run after each clustering refresh and will emit one drift event per run, including which clusters are impacted and which metrics exceeded thresholds.\n\n### Persistence model & audit trail  \nWe\u2019ll add a `drift_events` store with: `id`, `created_at`, `run_id`/`model_version`, `baseline_window`, `comparison_window`, `severity`, `metrics` (JSON), `affected_clusters` (array + per-cluster metric deltas), and `sample_conversation_ids` that explain the signal (e.g., top-N conversations with highest assignment change confidence or greatest embedding contribution). To support defensibility, we\u2019ll store stable references to the source conversations (conversation id + snapshot pointer or content hash) and allow attaching downstream work links (`linear_issue_ids` or generic `work_item_refs`). This keeps the audit trail lightweight while enabling \u201cwhy did we alert?\u201d drill-down without requiring full data duplication.\n\n### Alerting & UX / Ops integration  \nOn drift event creation at or above \u201ccritical,\u201d we\u2019ll publish a high-priority alert to our existing alerting channel (e.g., Slack/PagerDuty) and surface it in the internal triage UI as a banner with a link to the drift event detail page. The detail view will show the triggered metrics, affected clusters, and the sampled conversations, plus actions: acknowledge, mute (time-bounded), and \u201ccreate/link Linear ticket\u201d which backfills the audit trail. We\u2019ll include idempotency (one alert per run/event) and basic rate limiting to avoid alert storms during known migrations (e.g., embedding model upgrade), with explicit annotations on drift events when expected changes occur.",
      "link": "https://www.onyx.app/95255"
    }
  ],
  "primary_owners": [
    "kevin_sullivan"
  ],
  "secondary_owners": []
}