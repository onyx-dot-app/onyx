{
  "id": "69951a957a3a42f4a1684769d209ccd6",
  "semantic_identifier": "Design--Implement batching + backpressure controls in ingestion pipeline to prevent backlog blowups.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-06-29",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Implement batching + backpressure controls in ingestion pipeline to prevent backlog blowups.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview\nWe\u2019ll add explicit batching and backpressure controls to the ingestion pipeline so large backfills and bursty sources don\u2019t create unbounded queue growth or overwhelm the DB/downstream sync. The pipeline will be modeled as staged workers (fetch \u2192 parse \u2192 dedupe \u2192 persist), each with configurable batch sizing and concurrency limits, so throughput can be increased safely by tuning per-stage parameters rather than letting upstream production run ahead of downstream capacity.\n\n### Batching + concurrency limits\nEach stage will accept work in bounded batches (e.g., `fetch_batch_size`, `persist_batch_size`) and run with a capped parallelism (`fetch_concurrency`, `persist_concurrency`, etc.). Batching will be applied at natural boundaries: fetch aggregates source IDs/ranges, parse groups raw payloads, dedupe groups candidate records, and persist performs bulk upserts/transactions sized to DB limits. We\u2019ll implement per-stage semaphores and a bounded in-memory/queue buffer between stages to ensure upstream stages cannot enqueue indefinitely when downstream slows.\n\n### Backpressure signals and controls\nWe\u2019ll introduce backpressure based on (a) queue depth between stages and (b) retry/error rates (e.g., DB timeouts, downstream sync throttling). When thresholds are exceeded, the system will reduce effective intake by slowing fetch (token bucket/leaky bucket rate limiting), decreasing fetch concurrency, and/or pausing specific sources/backfill jobs until depth recovers; resume occurs automatically once the queue drops below a low-water mark (hysteresis to avoid flapping). Backpressure decisions will be centralized in a small \u201cingestion controller\u201d that periodically evaluates metrics and adjusts stage limiters, and will emit structured events for observability.\n\n### Idempotency and retry safety\nTo preserve idempotency under batching and throttling-induced retries, all persisted writes will use deterministic id keys and upsert semantics (or transactional \u201cinsert-if-not-exists\u201d patterns) and avoid side effects outside the DB transaction boundary. Work items will carry stable dedupe keys/checkpoints so reprocessing a batch (partially or fully) produces the same final state; acknowledgments will occur only after durable persistence, and partial failures will split/retry only the failed subset. We\u2019ll validate behavior with backfill-scale tests that force throttling and retries while asserting no duplicates, no missed records, and bounded queue growth.",
      "link": "https://www.onyx.app/86114"
    }
  ],
  "primary_owners": [
    "tyler_jenkins"
  ],
  "secondary_owners": []
}