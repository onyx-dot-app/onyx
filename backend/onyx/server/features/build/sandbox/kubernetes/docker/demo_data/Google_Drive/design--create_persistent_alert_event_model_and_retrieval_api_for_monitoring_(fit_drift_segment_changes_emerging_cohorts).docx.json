{
  "id": "8e92f4c9fb244a308abb9402c6256b88",
  "semantic_identifier": "Design--Create persistent alert event model + retrieval API for monitoring (fit drift/segment changes/emerging cohorts).docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2026-01-10",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Create persistent alert event model + retrieval API for monitoring (fit drift/segment changes/emerging cohorts).docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "This feature adds a persistent, normalized **AlertEvent** data model to store monitoring outputs (fit drift, segment changes, emerging cohorts) in a way that is queryable and stable across re-evaluations. Each event record will follow the schema defined in GOLD-40, capturing: `type`, normalized `entityRefs` (e.g., modelId, datasetId, segmentId/customerId), `eventTimestamp` (when the condition was detected) and `evaluationWindow` (start/end), `computedMetrics` (e.g., drift score, cohort size delta), `thresholds` used for the decision, and a deterministic `dedupeKey` to represent the \u201csame\u201d logical alert across runs.\n\nOn ingestion/write, the backend will expose an internal create/upsert path used by monitoring jobs. Writes must be **idempotent**: the system will compute/accept a `dedupeKey` and enforce a uniqueness constraint so repeated detections for the same window/type/entity collapse into a single event (either no-op or update `lastSeenAt`/`updatedAt` and any non-conflicting fields). This prevents notification spam and supports at-least-once job execution. Retention will default to a \u201clow-priority monitoring\u201d policy (e.g., time-based TTL/archival), with archived events still optionally retrievable for longer windows if needed.\n\nOn retrieval, we will implement query APIs to **list** and **fetch** alert events for downstream UI and notification delivery. The list endpoint will support filtering by `alertType`, `segmentId` and/or `customerId` (and other entity refs as needed), and `timeRange` (eventTimestamp or evaluationWindow overlap), returning results ordered by most recent detection. Pagination will use cursor-based pagination (stable sort key like `(eventTimestamp, id)`) to support infinite scroll and consistent paging under concurrent writes; responses will include `nextCursor` and total counts only if required (otherwise omitted for performance).\n\nOperationally, we\u2019ll include basic indexing to keep queries fast (e.g., composite indexes on `(type, eventTimestamp)`, `(customerId, eventTimestamp)`, `(segmentId, eventTimestamp)`, plus a unique index on `dedupeKey`). We will start with conservative defaults: store raw metrics/thresholds as JSON for flexibility, validate required fields per alert type, and log/metric write dedupe rates to verify idempotency behavior. This establishes a durable event backbone that can later power richer workflows (acknowledgement, suppression, linking to notifications) without changing the core event contract.",
      "link": "https://www.onyx.app/31286"
    }
  ],
  "primary_owners": [
    "andre_robinson"
  ],
  "secondary_owners": []
}