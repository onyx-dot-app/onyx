{
  "id": "ccebc107f91141008be5896af0f47107",
  "semantic_identifier": "Design--Wire calibration job to use user-defined outcome labels (no fallback to default).docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-08-02",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Wire calibration job to use user-defined outcome labels (no fallback to default).docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview\nToday the calibration pipeline can implicitly fall back to a \u201cdefault\u201d outcome when it cannot resolve user-configured outcome selection/labeling rules. This ticket makes the calibration job strictly honor the configured outcome rules for activation/retention/expansion end-to-end (feature generation + training label generation). If configuration is present and labels cannot be produced, the job must fail deterministically with a clear, user-visible error instead of silently training on a default outcome.\n\n### Proposed changes\nWe will thread the outcome configuration (selected outcome + label rule definitions) through the calibration job entrypoint into the fit-model calibration pipeline component that builds the training dataset. Label generation will be refactored to require an explicit outcome path (activation/retention/expansion) and to use only the configured label rules for that outcome when computing labels and any dependent features. Any previous \u201cdefault outcome\u201d branching will be removed/guarded such that it cannot execute when an outcome config is provided; the only allowed behavior is (a) produce labels per config or (b) fail.\n\n### Error handling and surfacing\nIf labels cannot be produced (e.g., outcome config missing required rules, rules resolve to no eligible events, or all labels are null/invalid), the pipeline should raise a typed error (e.g., `OutcomeLabelGenerationError`) that includes: selected outcome, a concise reason, and any relevant rule identifiers. The calibration job runner will treat this as a hard failure and surface the message in job status/logs in a way that is visible to the operator (and, if applicable, passed through to any UI/API status field). This ensures misconfiguration is caught early and prevents training on unintended labels.\n\n### Testing\nAdd a minimal integration test that feeds a fixed set of input signals/events into the pipeline and runs calibration three times\u2014once for activation, retention, and expansion\u2014using deterministic rule configs. The assertion is that each outcome path produces distinct, expected label outputs from the same inputs (and that no fallback/default label set is produced). Add one negative test case where configuration is present but cannot produce labels, asserting the job fails with the surfaced error and does not proceed to model fitting.",
      "link": "https://www.onyx.app/29824"
    }
  ],
  "primary_owners": [
    "kevin_sullivan"
  ],
  "secondary_owners": []
}