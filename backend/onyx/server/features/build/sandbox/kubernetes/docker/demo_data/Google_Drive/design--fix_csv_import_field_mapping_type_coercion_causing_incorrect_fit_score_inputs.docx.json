{
  "id": "6611e558f2bc48829de3eecc9dc4e2e1",
  "semantic_identifier": "Design--Fix CSV import field mapping/type coercion causing incorrect fit score inputs.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-05-02",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Fix CSV import field mapping/type coercion causing incorrect fit score inputs.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "We\u2019ll fix the `fit_estimation` CSV import path so the scoring service always receives correctly mapped and deterministically typed inputs. Today the importer can (a) bind the wrong CSV column to a target field due to ambiguous/fragile mapping and (b) coerce types inconsistently (e.g., `\"0\"`, `\"false\"`, empty strings, or numeric-like strings ending up as booleans/strings), which yields invalid driver inputs and misleading breakdowns. The goal is to make mapping explicit/validated, normalize values in a single canonical step, and prevent \u201csilent defaults\u201d that mask data quality issues.\n\nOn ingest, we\u2019ll introduce a strict mapping resolver that validates header-to-field mappings before processing rows. This will enforce: required fields present, no duplicate target fields, and only known headers (or approved aliases) are mapped. If mapping cannot be resolved unambiguously, the import will fail fast with a clear error describing the conflicting/missing columns and the expected schema. We\u2019ll also persist the resolved mapping (and importer version) with the import job to guarantee reproducibility and to support later debugging/auditing.\n\nFor row processing, we\u2019ll add a schema-driven normalizer that parses each mapped cell into a canonical typed representation (e.g., number, boolean, enum, string, null) using deterministic rules (trim, locale-agnostic number parsing, explicit boolean lexicon, empty\u2192null, enum validation). Unmappable values will not be coerced to defaults; instead we\u2019ll emit row-level validation errors identifying row number, column/header, target field, raw value, and expected type/allowed values. Rows with errors will be marked failed (or skipped, depending on existing behavior), and only fully normalized rows will be emitted as the \u201cscoring payload\u201d.\n\nFinally, we\u2019ll ensure the scoring service consumes the normalized payload (not raw CSV strings) by making the normalized object the single source of truth passed downstream and stored alongside the import results. We\u2019ll add tests covering tricky coercions (e.g., `\"001\"`, `\"1.0\"`, `\"TRUE\"`, `\"no\"`, `\" \"`), mapping edge cases (duplicate headers, alias collisions), and regression checks that driver breakdowns match inputs post-normalization. Observability will include counters for mapping failures, row-level type errors, and a small sampled log of error payloads to speed diagnosis without leaking full datasets.",
      "link": "https://www.onyx.app/12033"
    }
  ],
  "primary_owners": [
    "andre_robinson"
  ],
  "secondary_owners": []
}