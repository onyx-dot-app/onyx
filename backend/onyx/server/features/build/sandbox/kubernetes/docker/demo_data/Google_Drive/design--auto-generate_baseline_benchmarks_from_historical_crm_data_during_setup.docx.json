{
  "id": "98b5df730bce4209b91fe02db5984ff4",
  "semantic_identifier": "Design--Auto-generate baseline benchmarks from historical CRM data during setup.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-03-29",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Auto-generate baseline benchmarks from historical CRM data during setup.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview\nDuring initial setup, add an \u201cAuto-generate benchmarks\u201d step that scans the customer\u2019s historical CRM opportunity data (default lookback 12 months, minimum 6 months if data is sparse) and proposes baseline benchmarks used by our forecasting calibration and \u201cdeal at risk\u201d alerts. The system will compute suggested defaults such as average duration per stage, stage-to-stage conversion rates, overall win rate, and typical deal size; when segmentation fields (e.g., segment/region/plan) are present and sufficiently populated, we\u2019ll generate segmented benchmarks as well.\n\n### Data pipeline & calculations\nAfter CRM connection, we\u2019ll ingest opportunity stage history (timestamps per stage), closed outcomes, and amount plus optional segment fields. We\u2019ll compute: (1) stage duration distributions and suggested averages/medians per stage, (2) conversion rates between adjacent stages using cohorts of opportunities that entered a stage during the lookback window, (3) win rate by counting closed-won vs closed-lost among closed opportunities, and (4) deal size summaries (median + p75) overall and by segment when sample size thresholds are met. All computations should record metadata: lookback window, number of opportunities included, number excluded (missing stage history/amount), and last refresh time.\n\n### Admin UX & editability\nIn setup, present a table of proposed benchmarks with inline edit controls and a \u201ccoverage\u201d badge per metric (e.g., \u201cN=184 opps, 12 mo\u201d, \u201cSegmented: Mid-Market N=42\u201d). Provide a toggle to switch between overall and segmented views, and show warnings when coverage is low (below configurable N, or <6 months) with fallbacks to unsegmented values. Admins can accept all, edit individual numbers, or disable a benchmark category (e.g., ignore deal size). On save, we persist the final chosen values along with the metadata so future users can trace provenance.\n\n### Persistence, refresh, and consumers\nPersist benchmarks in our app DB as a versioned \u201cBenchmarkSet\u201d tied to the CRM connection/workspace, including per-metric values and coverage metadata; mark whether each value is auto-suggested vs admin-edited. Downstream services (forecast calibration and risk alerts) will read the latest active BenchmarkSet; risk scoring should use stage duration and conversion benchmarks, while forecast calibration uses win rate and deal size distributions. Optionally include a \u201cRecalculate from CRM\u201d action post-setup that generates a new draft BenchmarkSet without overwriting the active one until an admin accepts it.",
      "link": "https://www.onyx.app/28490"
    }
  ],
  "primary_owners": [
    "jason_morris"
  ],
  "secondary_owners": []
}