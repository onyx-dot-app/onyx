{
  "id": "https://github.com/onyx-dot-app/onyx#52",
  "semantic_identifier": "52: fix: Harden fit_estimation CSV import: deterministic field mapping + type normalization with row-level errors",
  "title": null,
  "source": "github",
  "doc_updated_at": "2025-05-20",
  "metadata": {
    "object_type": "PullRequest",
    "id": "52",
    "merged": "True",
    "state": "closed",
    "user": {
      "login": "sofia_ramirez",
      "name": "sofia_ramirez",
      "email": "sofia_ramirez@netherite-extraction.onyx.app"
    },
    "assignees": [
      {
        "login": "andre_robinson",
        "name": "andre_robinson",
        "email": "andre_robinson@netherite-extraction.onyx.app"
      }
    ],
    "repo": "netherite-ext/gold",
    "num_commits": "8",
    "num_files_changed": "66",
    "labels": [],
    "created_at": "2025-05-20",
    "updated_at": "2025-05-20",
    "closed_at": "2025-06-07"
  },
  "doc_metadata": {
    "repo": "netherite-ext/gold",
    "hierarchy": {
      "source_path": [
        "netherite-ext",
        "gold",
        "pull_requests"
      ],
      "owner": "netherite-ext",
      "repo": "gold",
      "object_type": "pull_request"
    }
  },
  "sections": [
    {
      "text": "## Description\r\n\r\n### Summary\nThis PR fixes issues in the fit_estimation CSV import pipeline where column-to-field mappings could be applied incorrectly and values could be coerced inconsistently (e.g., numeric fields ending up as strings/booleans). The import now performs deterministic mapping resolution, enforces explicit type normalization per target field, and records row-level validation errors for unmappable/invalid values instead of silently defaulting. The scoring service is fed the normalized payload produced by the importer to ensure fit score calculations and driver breakdowns reflect the intended inputs.\n\n### Key changes\n- **Mapping validation & resolution**\n  - Validates the resolved CSV column mapping against the expected fit_estimation schema (required fields present, no unknown target fields, no duplicate target assignments).\n  - Fails fast for invalid mappings (import-level error) before any scoring is triggered.\n\n- **Deterministic type normalization**\n  - Introduces a single normalization layer that coerces values based on the destination field type (number/int/boolean/string/enum as applicable).\n  - Handles common CSV representations consistently (e.g., \"TRUE\"/\"false\"/\"1\"/\"0\" for booleans; numeric strings with whitespace; empty strings -> null).\n  - Prevents implicit JS truthiness/loose parsing from turning values into unexpected types.\n\n- **Row-level error reporting (no silent defaults)**\n  - When a value cannot be normalized (e.g., \"N/A\" for a numeric field, invalid enum token), the row is marked with a structured error identifying the column, target field, and reason.\n  - Invalid rows are skipped from scoring input; valid rows proceed.\n\n- **Scoring input consistency**\n  - Ensures the scoring service consumes the importer\u2019s normalized/validated values (not raw CSV strings), eliminating mismatches between what users upload and what scoring uses.\n\n### User impact\n- Imports that previously \"succeeded\" while silently converting/guessing types may now surface explicit row-level errors.\n- Fit score calculations and driver breakdowns will be more reliable because inputs are normalized consistently and invalid values are excluded with clear diagnostics.\n\n### Testing\n- Added/updated unit tests for mapping validation, type coercion edge cases, and row-level error generation.\n- Added an integration-style test covering end-to-end import -> normalized payload -> scoring request payload shape.\r\n\r\n## How Has This Been Tested?\r\n\r\nstaging\r\n",
      "link": "https://github.com/onyx-dot-app/onyx#52"
    }
  ],
  "primary_owners": [],
  "secondary_owners": []
}