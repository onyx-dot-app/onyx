{
  "id": "64157b798af44c54b3e52cb58adfc9d4",
  "semantic_identifier": "Design--Zendesk/Intercom ingest: make conversation syncing idempotent to prevent duplicate issue evidence.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-04-11",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Zendesk/Intercom ingest: make conversation syncing idempotent to prevent duplicate issue evidence.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "We should make Zendesk/Intercom read-only ingest idempotent so retried syncs or overlapping time-window pulls never create duplicate conversation/message records or re-attach the same \u201cissue evidence\u201d to multiple issues. The core change is to treat each ingested unit (conversation, message, evidence attachment) as uniquely identified by an idempotency key derived from `(source, external_conversation_id, external_message_id)` (and for attachments, include the internal issue id if the relationship is its own record). With these keys, the ingest pipeline can safely re-run the same data and converge to a single canonical set of records.\n\nOn the data/model side, add stable columns for `source`, `external_conversation_id`, `external_message_id` (and/or a computed `idempotency_key`) and enforce uniqueness with a DB unique constraint over the appropriate tuple. Update the writer path to use upsert semantics (`INSERT \u2026 ON CONFLICT DO UPDATE`) rather than blind inserts, so if a record already exists we only update mutable fields (e.g., last_seen_at, updated_at, redacted text) and never create a new row. Apply the same pattern to the evidence linkage so a given message can only be attached once, and attachment updates are also idempotent.\n\nIn the ingest flow, perform dedupe/upsert before any downstream clustering or theme assignment. This means the clustering job should read only canonical conversation/message rows and stable evidence links, ensuring impacted-customer counts and theme frequencies don\u2019t inflate across sync runs. If clustering is triggered per-sync, gate it on \u201cnet new canonical records\u201d (e.g., rows inserted vs updated), or have clustering operate incrementally on the set of newly inserted ids returned by the upsert step.\n\nWe\u2019ll validate via integration tests that (1) two identical sync runs produce identical row counts and identical issue-evidence links, (2) overlapping time windows do not change totals, and (3) concurrent retries don\u2019t violate constraints (with retries succeeding after conflict). Add metrics/alerts for conflict rates and \u201cupdated vs inserted\u201d ratios per source, plus a backfill/cleanup script to collapse existing duplicates by selecting a canonical row per key, re-pointing foreign keys/evidence links, and deleting extras before enabling strict uniqueness constraints in production.",
      "link": "https://www.onyx.app/47511"
    }
  ],
  "primary_owners": [
    "brooke_spencer"
  ],
  "secondary_owners": []
}