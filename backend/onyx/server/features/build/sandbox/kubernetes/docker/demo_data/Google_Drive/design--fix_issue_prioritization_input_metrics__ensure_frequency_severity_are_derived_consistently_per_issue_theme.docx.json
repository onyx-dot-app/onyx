{
  "id": "a60a53df02fd4ffeb7db99a287bb541a",
  "semantic_identifier": "Design--Fix issue_prioritization input metrics: ensure frequency/severity are derived consistently per issue theme.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-05-01",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Fix issue_prioritization input metrics: ensure frequency/severity are derived consistently per issue theme.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Problem / Goal  \nPriority scoring for `issue_prioritization` is unstable because upstream producers send inconsistent `frequency` and `severity` for the same issue theme (e.g., frequency sometimes counts conversations, other times unique customers; severity defaults/normalizes differently depending on source). The goal is to make score inputs deterministic by standardizing metric derivation per issue theme before we call the scoring service, so the same underlying data always yields the same `frequency`/`severity` and therefore stable priority scores.\n\n### Proposed Approach  \nIntroduce a single \u201cIssueThemeMetricsDeriver\u201d step in the ingestion/aggregation pipeline that is the only component responsible for computing `frequency` and `severity` for an issue theme. **Frequency** will follow one counting rule: count unique customers impacted within a configured time window (e.g., last N days), where \u201cimpacted\u201d is determined after deduping events/conversations into the issue theme; the customer key is canonicalized (e.g., account_id, falling back to hashed email if needed) to ensure consistent uniqueness. **Severity** will be normalized to a single numeric scale (e.g., 1\u20135) via a shared mapping from source severities/labels; if no severity is present, apply a single default (e.g., 1/\u201clow\u201d) and record `severity_source = \"default\"` for auditability. The scoring service contract remains unchanged, but all callers must use this deriver output and stop passing source-native metrics directly.\n\n### Implementation Notes / Rollout  \nRefactor existing metric computation sites to route through the deriver (or remove duplicated logic) and update any schemas/events to include the canonicalized customer identifier and normalized severity fields used by the deriver. Add lightweight instrumentation to compare old vs. new inputs for a short period (shadow compute) and log deltas per issue theme to validate the counting rule and mapping correctness before fully switching. Roll out behind a feature flag with staged enablement (internal \u2192 small % \u2192 100%), and backfill/recompute theme metrics if necessary to avoid mixing legacy and new semantics in persisted aggregates.\n\n### Testing / Regression Protection  \nAdd unit tests for frequency counting (deduping behavior, uniqueness across multiple conversations, windowing) and severity normalization (mapping across all supported sources, missing severity default behavior). Add integration tests that run a small end-to-end pipeline fixture (multiple sources, overlapping customers, mixed severities) and assert deterministic `frequency`/`severity` outputs and stable computed score inputs. Finally, add a guard test around ingestion/deduping changes: given the same raw events, the deriver output must match a golden snapshot to catch accidental metric semantic drift.",
      "link": "https://www.onyx.app/55841"
    }
  ],
  "primary_owners": [
    "jason_morris"
  ],
  "secondary_owners": []
}