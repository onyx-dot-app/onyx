{
  "id": "b163223852ed4aeb9714c71bb277c550",
  "semantic_identifier": "Design--Optimize dedupe/cluster persistence queries and indexes for high-volume ingestion.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-08-05",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Optimize dedupe/cluster persistence queries and indexes for high-volume ingestion.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "We\u2019ll reduce ingestion latency and DB contention during high-volume backfills by profiling the dedupe/clustering path and the issue-object persistence path, then optimizing the highest-cost read/write patterns. The scope includes queries involved in: (1) locating existing issues/clusters by source ticket ID, conversation ID, and normalized fingerprint, and (2) persisting/merging issue objects and cluster memberships idempotently. We will not change dedupe semantics, merge outcomes, or idempotency guarantees; the goal is purely performance and operational stability under load.\n\nImplementation starts with production-like profiling on a backfill-sized dataset to identify the top \u201chot\u201d queries by total time, rows scanned, lock time, and frequency (e.g., via `EXPLAIN (ANALYZE, BUFFERS)` and DB metrics). For each hotspot, we\u2019ll rewrite queries to be index-friendly and reduce round trips: prefer targeted `SELECT ... FOR UPDATE SKIP LOCKED` (where appropriate) over broad scans, replace multi-step \u201ccheck then insert\u201d flows with upserts, and ensure lookups are done on normalized values used in predicates. Where the logic currently requires reading large ranges or joining on non-selective columns, we\u2019ll restructure into narrower key-based access patterns and batch writes to reduce transaction overhead.\n\nOn the schema side, we\u2019ll add/adjust indexes to match the access paths: composite indexes for frequent predicates (e.g., `(source_type, source_ticket_id)`, `(source_type, conversation_id)`, and `(project_id, normalized_fingerprint)` depending on current schema), plus unique constraints where they enforce idempotency (enabling `INSERT ... ON CONFLICT DO UPDATE/NOTHING`). If we have \u201cactive vs deleted\u201d semantics, we\u2019ll consider partial indexes to keep them small; if lookups are case/whitespace normalized, we\u2019ll ensure the stored normalized column is what\u2019s indexed (or use functional indexes if we can\u2019t store it). Any index additions will be evaluated for write amplification, built concurrently where supported, and rolled out with safe backfill/migration steps.\n\nValidation will be done by running an end-to-end ingest/backfill benchmark before and after, comparing throughput, p95/p99 transaction latency, scanned rows, and lock waits/deadlocks, and verifying that output is identical (same cluster assignments and merge results) across runs. We\u2019ll add regression coverage for idempotent replays (re-ingesting the same backlog, partial retries, and concurrent workers) and add lightweight runtime instrumentation (query timing and conflict rates) to ensure the improvements hold in production and don\u2019t regress over time.",
      "link": "https://www.onyx.app/59059"
    }
  ],
  "primary_owners": [
    "jason_morris"
  ],
  "secondary_owners": []
}