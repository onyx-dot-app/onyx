{
  "id": "1a83893eb7a343be9f607bb5ddb44d88",
  "semantic_identifier": "Design--Customer_Analyzer v1: Backfill + scheduled refresh for segment IDs and fit tiers on existing accounts.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-07-14",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Customer_Analyzer v1: Backfill + scheduled refresh for segment IDs and fit tiers on existing accounts.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Customer_Analyzer v1 Backfill + Scheduled Refresh (Segment IDs + Fit Tiers)\n\nWe will add a one-time backfill job that scans all existing `Issue_Tracker` accounts that appear in previously ingested conversations and populates two derived fields: `customer_analyzer_segment_id` and `customer_analyzer_fit_tier`. The backfill will be resilient to incomplete data by explicitly supporting `unknown`/`missing` states (e.g., nullable columns plus an enum/status field, or sentinel values) and will never block ingestion or other pipelines. The job will process accounts in batches, with a stable ordering and checkpointing (cursor by account_id / updated_at) so it can be paused/resumed and re-run safely.\n\nFor ongoing freshness, we will implement a scheduled refresh (daily cron) that selects eligible accounts using a per-account TTL based on `last_fetched_at` (e.g., refresh if `now - last_fetched_at > 24h`). This selection should also exclude accounts currently in-flight to avoid duplicate work, and apply conservative rate limiting/concurrency caps to prevent hammering Customer_Analyzer (e.g., fixed QPS budget + max concurrent requests). Each refresh updates `segment_id`, `fit_tier`, and `last_fetched_at` only when the upstream call succeeds; transient failures should not advance `last_fetched_at` so the account can retry on the next run/backoff window.\n\nIdempotency will be guaranteed by making updates conditional and repeatable: use upserts keyed by `account_id`, and treat the fetched payload as the source of truth (writing the same values repeatedly is safe). To avoid oscillation or accidental nulling, the updater will follow a \u201cdo no harm\u201d rule: if Customer_Analyzer returns missing fields, we store `unknown` while preserving the previous known value only if we can distinguish \u201cunknown due to error\u201d vs \u201cunknown is the true value\u201d (recommended: store both `value` and `value_status`, where status is `known|unknown|error`). We will also write defensive handling for deleted/merged accounts and record \u201cnot found\u201d as a stable terminal status to prevent endless retries.\n\nFinally, we will add basic observability: structured logs per batch/run, counters for attempted/succeeded/failed accounts, and breakdowns by failure class (timeout, 4xx, 5xx, not_found, parse_error). Emit metrics such as `backfill.accounts_processed`, `refresh.accounts_refreshed`, `customer_analyzer.request_latency_ms`, and `customer_analyzer.error_rate`, plus a daily summary log. This will let us confirm coverage across historical accounts, validate that refresh keeps data reasonably current, and tune TTL/rate limits if we see undue load or low success rates.",
      "link": "https://www.onyx.app/60834"
    }
  ],
  "primary_owners": [
    "jiwon_kang"
  ],
  "secondary_owners": []
}