{
  "id": "2f97596c0234438ab72e8c6ad02733c5",
  "semantic_identifier": "Design--Define and persist bypass-tracking events in backend analytics pipeline.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-10-31",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Define and persist bypass-tracking events in backend analytics pipeline.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Overview\nWe will add first-class backend analytics support for Sales_Accelerator \u201cbypass\u201d actions by defining a dedicated event schema, exposing an ingestion endpoint, and persisting these events into the analytics warehouse with stable identifiers. The goal is to enable reliable reporting and segmentation by workflow and team, and to support downstream dashboards/alerting without requiring any UI changes. This work standardizes bypass tracking across surfaces and ensures the data is durable and queryable.\n\n### Event schema\nIntroduce a `sales_accelerator_bypass` event with a stable, versioned schema (e.g., `event_name`, `schema_version`). Required fields: `event_id` (UUID), `occurred_at` (timestamp), `user_id`, `team_id`, `workflow_type` (enum), `surface` (enum), and one of `{account_id, opportunity_id}` (or both when applicable). Optional fields: `reason_code` (enum) and `reason_text` (string), plus `session_id` (string) to aid dedupe and funnel analyses. All identifiers should use canonical backend IDs (not display names) and include additional stable dimensions where needed (e.g., `org_id` if that\u2019s the tenant boundary).\n\n### Ingestion + idempotency\nAdd a dedicated backend endpoint (e.g., `POST /analytics/events/sales-accelerator/bypass`) that accepts the schema payload and forwards it into the existing analytics pipeline. Enforce validation (required fields, enum values, timestamp bounds) and implement idempotency by deduplicating on `event_id` (primary) and optionally `(session_id, user_id, workflow_type, occurred_at_bucket)` as a secondary guardrail for retried clients. The ingestion path should be resilient to retries (at-least-once delivery), returning success on duplicates and emitting metrics/logging for invalid payloads and rate anomalies.\n\n### Warehouse storage + downstream use\nPersist into a dedicated warehouse table (e.g., `fact_sales_accelerator_bypass_events`) partitioned by `occurred_at` and clustered by `team_id/workflow_type` for efficient segmentation. Store raw event payload (for forward compatibility) alongside typed columns for key dimensions, and maintain stable identifiers to support consistent joins to user/team/account/opportunity dimensions. Ensure the events are available in the standard downstream layer (modeled views or semantic layer) with a clear SLA and data freshness expectations, and add basic data quality checks (volume, null rates, dedupe rate) to support dashboards and alerting.",
      "link": "https://www.onyx.app/84605"
    }
  ],
  "primary_owners": [
    "brooke_spencer"
  ],
  "secondary_owners": []
}