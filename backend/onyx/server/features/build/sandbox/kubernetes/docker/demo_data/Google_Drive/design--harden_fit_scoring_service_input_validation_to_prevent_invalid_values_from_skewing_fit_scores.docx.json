{
  "id": "c0f1707626624a6f83d3921ed3f0cdec",
  "semantic_identifier": "Design--Harden fit scoring service input validation to prevent invalid values from skewing fit scores.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-03-31",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Harden fit scoring service input validation to prevent invalid values from skewing fit scores.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "We should harden the fit scoring service by adding strict, explicit input validation at the scoring boundary (the API/worker entrypoint that receives normalized customer records). Today we accept missing/NaN/out-of-range values and unexpected enums and then silently coerce or default them, which produces misleading overall fit scores and incorrect driver breakdowns. The goal is to ensure that only well-formed, in-domain values influence scoring, and that any invalid inputs are surfaced to the caller as actionable validation feedback.\n\nImplementation-wise, we\u2019ll define a versioned schema for the scoring request (including numeric ranges, required/optional fields, and allowed enum values) and validate every record against it before scoring. Validation should distinguish between (a) record-level fatal issues (e.g., missing required identifiers) that prevent scoring for that record, and (b) field-level issues (e.g., NaN, out-of-range, unknown enum) that should cause that specific field to be excluded from scoring while still allowing the rest of the record to be scored. The scorer will be updated to treat excluded fields as \u201cunset\u201d (not defaulted) so they do not contribute to the score or driver calculations; driver breakdown should reflect \u201cnot scored due to invalid input\u201d rather than a misleading defaulted contribution.\n\nThe service response should include both the computed score outputs and a structured validation payload per record: which fields failed, the error type (missing/NaN/out_of_range/unknown_enum), the received value, and (when applicable) the expected constraints (range, allowed values). For record-level fatal validation failures, we return a clear status for that record (e.g., `rejected`) with no score, while still returning scores for other valid records in the same request (partial success). This keeps scoring non-blocking for otherwise-valid customers while making invalid data visible and debuggable.\n\nOperationally, we\u2019ll add metrics and logging around validation failures (rate by field/error type, top offenders, and rejection counts) and consider a temporary \u201cwarn-only\u201d rollout mode to quantify impact before enforcing strict rejection. We\u2019ll also document the schema and error codes for callers (including CSV import) so they can remediate upstream. Success is measured by eliminating silent coercions, reducing misleading fit scores, and providing consistent per-field validation errors that enable quick correction without degrading throughput or availability.",
      "link": "https://www.onyx.app/40250"
    }
  ],
  "primary_owners": [
    "jason_morris"
  ],
  "secondary_owners": []
}