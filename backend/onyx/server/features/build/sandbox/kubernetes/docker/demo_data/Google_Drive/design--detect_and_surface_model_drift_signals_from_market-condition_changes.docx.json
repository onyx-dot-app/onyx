{
  "id": "0b57f8c18fdd4639aee73f7afd1f65a1",
  "semantic_identifier": "Design--Detect and surface model drift signals from market-condition changes.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-10-17",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Detect and surface model drift signals from market-condition changes.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "### Summary / Goal\nWe will add drift monitoring to the core Sales_Accelerator ML models (forecasting, best-next-action, and risk alerts) to detect market-condition changes that degrade model reliability. The system will compute recurring drift signals from (a) feature distribution shifts and (b) prediction/label stability changes, then surface configurable alerts with segment/context breakdowns to help operators quickly identify likely drivers (e.g., pricing changes, competitor mentions, packaging updates). Drift status will be exposed via an internal endpoint and structured logs to support downstream recalibration workflows.\n\n### Scope / Approach\nFor each model, we will define a monitored feature set (inputs + key derived features) and model outputs (scores, classes, forecast residuals) along with a baseline window (e.g., trailing 30\u201360 days) and a current window (e.g., last 1\u20137 days). Feature drift will be measured using appropriate metrics by type (PSI/JS divergence for numeric binned features, KL/chi-square for categorical, missingness-rate changes for all). Output drift will include prediction distribution shifts and, where labels are available, changes in error/residual distributions (e.g., MAE/MAPE deltas for forecasting; calibration/precision-recall deltas for classification). All metrics will be computed globally and by key segments (region, industry, deal size tier, product line, channel), with per-segment sample-size guards to avoid noisy alerts.\n\n### Alerting + Explainability\nAlerts will trigger when drift metrics cross configurable thresholds for N consecutive runs and exceed minimum sample sizes; thresholds will be stored per model/metric/segment and tuned with initial defaults. Each alert payload will include: affected model/version, metric values vs baseline, top contributing features (ranked by drift magnitude), most impacted segments, and a \u201clikely drivers\u201d section populated by mapped feature groups (e.g., pricing-related features, competitor-mention NLP features, packaging/SKU features) and recent changes in those groups. Alerts will route to the existing on-call/ops channels (e.g., Slack/PagerDuty/email) and be deduplicated via a cooldown window to prevent flooding during sustained regime shifts.\n\n### Exposure / Interfaces / Deliverables\nWe will publish drift snapshots to a durable store (e.g., time-series table) and emit structured logs for each run, enabling audit and analysis. An internal read-only endpoint will expose the latest drift status per model and recent history (e.g., `/internal/ml/drift/{model}`) for use by calibration tooling and dashboards; it will return metric summaries, segment drilldowns, and last alert metadata. Initial delivery includes: scheduled job(s) to compute drift, config schema + defaults, alert routing, endpoint + logging, and a runbook describing interpretation and next steps (e.g., when to retrain, recalibrate, or adjust thresholds).",
      "link": "https://www.onyx.app/19409"
    }
  ],
  "primary_owners": [
    "brooke_spencer"
  ],
  "secondary_owners": []
}