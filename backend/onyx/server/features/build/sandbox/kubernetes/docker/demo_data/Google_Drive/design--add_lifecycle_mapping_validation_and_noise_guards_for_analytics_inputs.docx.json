{
  "id": "d253c42ee1e94672806c01cfb88b87d2",
  "semantic_identifier": "Design--Add lifecycle mapping validation + noise guards for analytics inputs.docx",
  "title": null,
  "source": "google_doc",
  "doc_updated_at": "2025-07-29",
  "metadata": {
    "owner_names": ""
  },
  "doc_metadata": {
    "hierarchy": {
      "source_path": [
        "My Drive"
      ],
      "drive_id": null,
      "file_name": "Design--Add lifecycle mapping validation + noise guards for analytics inputs.docx",
      "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    }
  },
  "sections": [
    {
      "text": "Add a validation step to the CRM ingestion/normalization pipeline that runs per connected CRM instance to ensure lifecycle/stage mappings are complete and unambiguous before records are admitted into lifecycle-dependent analytics and forecasting. The validator will compare incoming CRM records (including custom stage names and required fields) against the instance\u2019s configured mapping table and required-field schema, and will classify failures into a small set of explicit reasons (e.g., `unmapped_stage`, `ambiguous_stage_mapping`, `missing_required_field:<field>`, `invalid_field_value:<field>`). This runs after raw ingestion but before canonical lifecycle assignment/aggregation so we can guard downstream metrics without blocking ingestion entirely.\n\nOn validation failure, we will still persist the record in the normalized store, but attach a `mapping_error` annotation (e.g., `record.flags.mapping_error=true` plus `record.mapping_error_reason` and optional `record.mapping_error_detail` such as the offending stage name). Records with `mapping_error` will be excluded from lifecycle-based analytics/forecasting aggregates and any derived tables/materializations that assume valid lifecycle state; other non-lifecycle features can continue to use the record. For safety, exclusion should be implemented as a consistent predicate in the aggregation layer (e.g., `WHERE mapping_error IS NULL`) and propagated to caches/rebuild jobs so backfills don\u2019t reintroduce noise.\n\nTo make issues actionable, emit structured logs and metrics keyed by `{crm_instance_id, crm_type, field_or_stage, error_reason}` with counts and example values (bounded sampling) to help sales ops pinpoint exactly which mapping is drifting (e.g., a newly added stage name that isn\u2019t mapped). Add dashboards/alerts on sustained error-rate increases per instance and on newly observed unmapped stage values, and include an operator-facing report endpoint or admin UI panel that lists top offenders and last-seen timestamps to accelerate remediation.\n\nRollout will be gated behind a feature flag per CRM instance with a \u201cshadow mode\u201d that only logs/metrics without excluding records, followed by \u201cenforced mode\u201d that applies the exclusion predicate. We\u2019ll add unit tests for mapping edge cases (custom stages, multiple matches, missing fields), integration tests validating aggregates exclude flagged records, and a backfill job to revalidate historical records when mappings are updated (clearing `mapping_error` automatically when a previously invalid record becomes valid). Success is measured by reduced volatility in lifecycle KPIs while maintaining observability into the small set of records being excluded.",
      "link": "https://www.onyx.app/93543"
    }
  ],
  "primary_owners": [
    "kevin_sullivan"
  ],
  "secondary_owners": []
}