# Onyx Evaluations

This directory contains the evaluation framework for testing and measuring the performance of Onyx's chat and retrieval systems.

## Overview

The evaluation system uses [Braintrust](https://www.braintrust.dev/) to run automated evaluations against test datasets. It measures the quality of responses generated by Onyx's chat system and can be used to track performance improvements over time.

## Prerequisites

**Important**: The model server must be running in order for evals to work properly. Make sure your model server is up and running before executing any evaluations.

## Running Evaluations

Kick off a remote job
```bash
onyx/backend$ python onyx/evals/eval_cli.py --remote --api-key <SUPER_CLOUD_USER_API_KEY> --search-permissions-email <email account to reference> --remote --remote-dataset-name Simple
```

You can also run the CLI directly from the command line:

```bash
onyx$ python -m dotenv -f .vscode/.env run -- python backend/onyx/evals/eval_cli.py --local-dataset-path backend/onyx/evals/data/eval.json --search-permissions-email richard@onyx.app
```
make sure your AUTH_TYPE=disabled when running evals locally. Save the env var ONYX_EVAL_API_KEY in your .env file so you don't 
have to specify it every time for triggering remote runs. 


### Production Environment

### Local Development

For local development, use the `eval_cli.py` script. We recommend starting it from the VS Code launch configuration for the best debugging experience.

#### Using VS Code Launch Configuration

1. Open VS Code in the project root
2. Go to the "Run and Debug" panel (Ctrl/Cmd + Shift + D)
3. Select "Eval CLI" from the dropdown
4. Click the play button or press F5

This will run the evaluation with the following default settings:
- Uses the local data file at `evals/data/data.json`
- Enables verbose output
- Sets up proper environment variables and Python path

#### CLI Options

- `--local-data-path`: Path to local JSON file containing test data (defaults to `evals/data/data.json`)
- `--remote-dataset-name`: Name of remote Braintrust dataset
- `--braintrust-project`: Braintrust project name (overrides `BRAINTRUST_PROJECT` env var)
- `--verbose`: Enable verbose output

## Test Data

The evaluation system uses test data stored in `evals/data/data.json`. This file contains a list of test cases, each with:
- `input`: The question or prompt to test
- `research_type`: DEEP or THOUGHTFUL

Example test case:
```json
{
    "input": { 
      "message": "What is the capital of France?",
      "research_type": "THOUGHTFUL"
    }
}
```
