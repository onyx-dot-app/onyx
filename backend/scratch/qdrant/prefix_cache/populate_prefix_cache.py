"""
Script to populate the prefix_cache collection with common query prefixes.

This pre-computes embeddings for frequently typed prefixes to accelerate
search-as-you-type functionality.
"""

import os
from pathlib import Path

import cohere
from dotenv import load_dotenv
from fastembed import SparseTextEmbedding
from qdrant_client.models import PointStruct
from qdrant_client.models import SparseVector

from scratch.qdrant.client import QdrantClient
from scratch.qdrant.prefix_cache.prefix_to_id import prefix_to_id
from scratch.qdrant.schemas.collection_name import CollectionName


def get_common_prefixes() -> list[str]:
    """
    Get common query prefixes extracted from the actual corpus.

    Loads ~10k most popular prefixes (1-5 chars) from corpus analysis.
    Generated by extract_all_prefixes.py from target_docs.jsonl.

    Returns:
        List of prefix strings to cache
    """
    # Load prefixes from file (too large to hardcode)
    prefix_file = Path(__file__).parent / "corpus_prefixes_100k.txt"

    if not prefix_file.exists():
        raise FileNotFoundError(
            f"Prefix file not found: {prefix_file}\n"
            "Run: python -m scratch.qdrant.prefix_cache.extract_all_prefixes"
        )

    with open(prefix_file, "r") as f:
        prefixes = [line.strip() for line in f if line.strip()]

    print(f"Loaded {len(prefixes):,} prefixes from {prefix_file.name}")

    return prefixes


def populate_prefix_cache() -> None:
    """
    Populate the prefix_cache collection with pre-computed embeddings.
    """
    # Load environment variables
    env_path = Path(__file__).parent.parent.parent.parent.parent / ".vscode" / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"Loaded environment variables from {env_path}")
    else:
        print(f"Warning: .env file not found at {env_path}")

    print("=" * 80)
    print("POPULATING PREFIX CACHE")
    print("=" * 80)

    # Initialize clients
    client = QdrantClient()
    cohere_api_key = os.getenv("COHERE_API_KEY")
    if not cohere_api_key:
        raise ValueError("COHERE_API_KEY environment variable not set")

    cohere_client = cohere.Client(cohere_api_key)
    # Use BM25 for sparse embeddings
    sparse_model = SparseTextEmbedding(model_name="Qdrant/bm25", threads=2)

    # Get prefixes to cache
    prefixes = get_common_prefixes()
    print(f"\nGenerating embeddings for {len(prefixes)} prefixes...")

    # Generate embeddings in batches
    batch_size = 96  # Cohere API batch limit
    points = []

    for i in range(0, len(prefixes), batch_size):
        batch = prefixes[i : i + batch_size]
        print(f"\nProcessing batch {i // batch_size + 1} ({len(batch)} prefixes)...")

        # Generate dense embeddings with Cohere
        print("  Generating dense embeddings...")
        dense_response = cohere_client.embed(
            texts=batch,
            model="embed-english-v3.0",
            input_type="search_query",
        )

        # Generate sparse embeddings
        print("  Generating sparse embeddings...")
        sparse_embeddings = list(sparse_model.query_embed(batch))

        # Create points - Convert prefix to u64 integer point ID
        for prefix, dense_emb, sparse_emb in zip(
            batch, dense_response.embeddings, sparse_embeddings
        ):
            point_id = prefix_to_id(prefix)  # Convert prefix to u64 integer!

            point = PointStruct(
                id=point_id,  # u64 integer encoding of the prefix
                vector={
                    "dense": dense_emb,
                    "sparse": SparseVector(
                        indices=sparse_emb.indices.tolist(),
                        values=sparse_emb.values.tolist(),
                    ),
                },
                payload={
                    "prefix": prefix,  # Store original prefix for reference
                    "hit_count": 0,
                },
            )
            points.append(point)

        # Upload this batch immediately (don't accumulate all points)
        print("  Uploading batch to Qdrant...")
        client.override_points(
            points=points[i : i + len(batch)],  # Upload just this batch
            collection_name=CollectionName.PREFIX_CACHE,
        )
        print(f"  ✓ Batch uploaded ({i + len(batch)} / {len(prefixes)} total points)")

    print("\n✓ Successfully uploaded all prefix cache entries")

    # Verify
    collection_info = client.get_collection(CollectionName.PREFIX_CACHE)
    print("\nCollection status:")
    print(f"  Points count: {collection_info.points_count}")
    print(f"  Status: {collection_info.status}")

    print("\n" + "=" * 80)
    print("PREFIX CACHE POPULATION COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    populate_prefix_cache()
